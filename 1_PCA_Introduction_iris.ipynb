{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is largely reproduced from \n",
    "\n",
    "- [PCA clearly explained â€”When, Why, How to use it and feature importance: A guide in Python](https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e)\n",
    "- [Principal Component Analysis Visualization by Prasad Ostwal](https://ostwalprasad.github.io/machine-learning/PCA-using-python.html)\n",
    "- [PCA in 3 steps by Sebastian Raschka](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n",
    "- [PCA by plotly](https://plot.ly/ipython-notebooks/principal-component-analysis/)\n",
    "- [In Depth: Principal Component Analysis](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "- A family of unsupervised machine learning technigies. You're already familiar with unsupervised ML (e.g., clustering analysis, K-Means algorithm)\n",
    "- **Reducing the number of variables (or the dimension of your data) to a more manageable set of variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family of Machine Learning Algorithms\n",
    "\n",
    "<img src='images/machinelearning.gif' height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boston.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **convert 13 columns to two dimensions or components**. Here is the visual presentation.\n",
    "\n",
    "<img src='https://ostwalprasad.github.io/images/p2/output_20_1.png'>\n",
    "\n",
    "Now we can see **how influential each of the original variables was in each component**.\n",
    "\n",
    "<img src='https://ostwalprasad.github.io/images/p2/output_26_0.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pay attention to PTRATIO and CRIM\n",
    "sns.heatmap(boston.corr(), vmax=.8, square=True, annot=True, fmt=\".1f\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms for Dimentionality Reduction: [scikit-learn](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)\n",
    "\n",
    "1. **Principal component analysis (PCA)**\n",
    "2. Truncated singular value decomposition and latent semantic analysis\n",
    "3. Dictionary Learning\n",
    "4. Factor Analysis\n",
    "5. Independent component analysis (ICA)\n",
    "6. Non-negative matrix factorization (NMF or NNMF)\n",
    "7. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.express as px  #if you don't have this, install first\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: iris\n",
    "\n",
    "This data has four variables and let's try to reduce to two dimensions. \n",
    "\n",
    "<img src=\"images/iris.png\" height=25 width=200>\n",
    "<img src=\"images/iris_3.gif\" height=300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']].values\n",
    "y = df['Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing or normalizing the features \n",
    "x = StandardScaler().fit_transform(X)\n",
    "pd.DataFrame(x).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Projection to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(x)\n",
    "\n",
    "fig = px.scatter(components, x=0, y=1, color=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf = pd.DataFrame(data = components, columns = ['pca1', 'pca2'])\n",
    "pcadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([pcadf, df[['Name']]], axis = 1)\n",
    "finalDf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance\n",
    "\n",
    "*The explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as while you can convert 4 dimensional space to 2 dimensional space, you lose some of the variance (information) when you do this*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)            # explained variance of each component\n",
    "print(pca.explained_variance_ratio_.cumsum())   # cumulative sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Loadings\n",
    "\n",
    "Visualize how strongly each characteristic influences a principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(components, x=0, y=1, color=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "fig = px.scatter(components, x=0, y=1, color=y)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    fig.add_shape(\n",
    "        type='line',\n",
    "        x0=0, y0=0,\n",
    "        x1=loadings[i, 0],\n",
    "        y1=loadings[i, 1]\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=loadings[i, 0],\n",
    "        y=loadings[i, 1],\n",
    "        ax=0, ay=0,\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"bottom\",\n",
    "        text=feature,\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SepalLength, PetalLength, and PetalWidth** are the most important for PC1\n",
    "- **SepalWidth** is the most important for PC2\n",
    "- **Arrows** (variables/features) that point into the same direction indicate **correlation** between the variables that they represent whereas, the arrows heading in **opposite directions** indicate a contrast between the variables they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above findings are verified using correlation analysis\n",
    "df[features].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of variables on each components\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11,8)})\n",
    "\n",
    "ax = sns.heatmap(pca.components_,\n",
    "                 cmap='YlGnBu',\n",
    "                 yticklabels=[ \"PCA\"+str(x) for x in range(1,pca.n_components_+1)],\n",
    "                 xticklabels=list(df[features].columns),\n",
    "                 cbar_kws={\"orientation\": \"horizontal\"})\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Projection to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(x)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=y,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics\n",
    "\n",
    "Reproduced from [PCA from scratch](https://towardsdatascience.com/principal-component-analysis-pca-from-scratch-in-python-7f3e2a540c51) and [PCA with Numpy](https://towardsdatascience.com/pca-with-numpy-58917c1d0391)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix\n",
    "\n",
    "Covariance matrices, like correlation matrices, contain information about the amount of variance shared between pairs of variables.\n",
    "\n",
    "Covariance shows **the direction (positivity, negativity) of the relationship**, rather than its strength (correlation). The covariance values are not standardized so go beyond -1 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the covariance matrix\n",
    "covariance_matrix = np.cov(x.T)\n",
    "covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors & Eigenvalues\n",
    "\n",
    "- Eigenvectors: principal components, detemining the directions of the new feature space \n",
    "- Eigenvalue: determining their magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)\n",
    "print(\"Eigenvector: \\n\", eigen_vectors,\"\\n\")\n",
    "print(\"Eigenvalues: \\n\", eigen_values, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Eigenvectors are the principal components**. The **first principal component is the first column with values of 0.52, -0.26, 0.58, and 0.56**. The second principal component is the second column and so on. \n",
    "- **Each Eigenvector will correspond to an Eigenvalue**, each eigenvector can be scaled of its eigenvalue, whose magnitude indicates how much of the dataâ€™s variability is explained by its eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from this, we can calculate the percentage of explained variance per principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the explained variance on each of components\n",
    "variance_explained = []\n",
    "for i in eigen_values:\n",
    "     variance_explained.append((i/sum(eigen_values))*100)\n",
    "        \n",
    "print(variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying components that explain at least 95%\n",
    "cumulative_variance_explained = np.cumsum(variance_explained)\n",
    "print(cumulative_variance_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two principal components account for around 96% of the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the eigenvalues and finding the \"elbow\" in the graphic\n",
    "sns.lineplot(x = [1,2,3,4], y=cumulative_variance_explained)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"Explained variance vs Number of components\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
